{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data-Mining Course (EECS 6412)\n",
        "# Assignment (II): Decision Tree Classifier Implementation in Python\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0yUOzXh0KEYf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Objective: Implement a Decision Tree classifier in Python to gain a deeper understanding of its working principles.\n",
        "\n",
        "**Overal Instructions:**\n",
        "\n",
        "\n",
        "*   Your task is to implement a Decision Tree classifier in Python.\n",
        "*   The implementation has been broken down into multiple subfunctions, each with accompanying hints. Your goal is to complete the code for each function.\n",
        "* You are only allowed to use the **pandas** and **numpy** libraries for this assignment. Some functions from Pandas have been provided for your convenience in the initial section, and you may use them if you feel they are necessary.\n",
        "* Each part of your solution will be graded separately. However the sections are interrelated. It is crucial that your code is well-documented with comments explaining each part of your implementation.\n",
        "* Please be aware that your responses will be thoroughly reviewed to ensure originality. Plagiarized or copied work will result in penalties.\n",
        "\n",
        "\n",
        "**- Please skip the following descriptions and move directly to the Questions section if you are familiar with reading CSV files with Pandas library.**"
      ],
      "metadata": {
        "id": "065A5OW9KOFA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "##Please write your full name/names and student IDs here:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   Full Name:\n",
        "*   Student ID:\n"
      ],
      "metadata": {
        "id": "XVp8WM0eDdDa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## Dataset Description for \"Car Acceptability\" Classification:\n",
        "Your codes should be general and must work on each tabular dataset with combined categorical and numerical data types. For this example, you have been provided with three datasets for training, testing and validation. Please download datasets from [here](https://drive.google.com/drive/folders/1uSr0rvbp2dExYRTDxL5vAouLXH1fJsOH?usp=sharing). These samples represent the decisions of car experts regarding the acceptability of cars. The experts have categorized the cars into one of four classes: \"acceptable,\" \"unacceptable,\" \"good,\" or \"very good\" based on five categorical features and one numerical feature.\n",
        "\n",
        "# Features:\n",
        "\n",
        "* **'BUYING':** An intiger representing the purchase price of the car.\n",
        "\n",
        "* **'MAINTENANCE':** This feature indicates how high the car's maintenance cost is, and it is categorized into four classes: 'vhigh' (very high), 'high', 'med' (medium), or 'low'.\n",
        "\n",
        "* **'DOORS':** This featurte indicates number of the doors each car has: '2', '3', '4', '5more'(5 or more than 5 doors).\n",
        "\n",
        "* **'PERSONS':** This feature determines the car's capacity in terms of the number of persons it can accommodate and is categorized as '2', '4', or 'more'.\n",
        "\n",
        "* **'LUG_BOOT':** This feature represents the size of the car's luggage boot (trunk) and is categorized as 'small', 'med' (medium), or 'big'.\n",
        "\n",
        "* **'SAFETY':** This feature provides an estimate of the car's safety level and is categorized as 'low', 'med' (medium), or 'high'.\n",
        "\n",
        "* **'CLASS':** This is the target variable. It indicates the acceptance level of the car and is categorized as 'unacc' (unacceptable), 'acc' (acceptable), 'good', or 'vgood' (very good).\n",
        "\n",
        "**Please note that in this example the \"CLASS\" attribute is located at the last column of the tabular datasets**\n"
      ],
      "metadata": {
        "id": "UBJOIRZSOiDd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Accessing the Datasets:\n",
        "To access and read datasets from Google Drive in Google Colab using the Pandas library, you can follow these steps:\n",
        "\n",
        "1.   Upload CSV Files to Google Drive: First, ensure that you've uploaded the CSV files (train dataset and test dataset) to your Google Drive. You can create a folder for your project and upload the files there.\n",
        "\n",
        "\n",
        "2.   Mount Google Drive in Google Colab:mount your Google Drive using the following code:\n"
      ],
      "metadata": {
        "id": "t8XuTyhMnwP5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AteP7IIqn4XI",
        "outputId": "4a00fb2a-d976-4731-bae1-0e476e26e87a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "3.   Access and Read Data using Pandas: You can access your CSV files in the mounted Google Drive directory. For example, if your CSV files are located in a folder named \"data_mining_assignment2/assignment_grads/Fall2025/datasets\" in your Google Drive, you can read them as follows:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "T7tuWcl8zVkM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the file paths for your CSV files\n",
        "train_csv_path = '/content/drive/MyDrive/data_mining_assignment2/assignment_grads/Fall2025/datasets/data_train_cn.csv'\n",
        "test_csv_path = '/content/drive/MyDrive/data_mining_assignment2/assignment_grads/Fall2025/datasets/data_test_cn.csv'\n",
        "validation_csv_path = '/content/drive/MyDrive/data_mining_assignment2/assignment_grads/Fall2025/datasets/data_validation_cn.csv'\n",
        "\n",
        "# Read the data into Pandas DataFrames\n",
        "train_df = pd.read_csv(train_csv_path)\n",
        "test_df = pd.read_csv(test_csv_path)\n",
        "validation_df = pd.read_csv(validation_csv_path)"
      ],
      "metadata": {
        "id": "w6jZmiO5zwrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "4.   See Some Samples with head() Function:\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "m34_HjXP2PZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# See the first 5 samples in the training dataset\n",
        "print(\"Samples in the Training Dataset:\")\n",
        "print(train_df.head())\n",
        "\n",
        "# See the first 5 samples in the test dataset\n",
        "print(\"\\nSamples in the Test Dataset:\")\n",
        "print(test_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abW4fCsV2U6O",
        "outputId": "d1fcc7bc-da61-4967-c770-e6894624e863"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Samples in the Training Dataset:\n",
            "   BUYING MAINTENANCE  DOORS PERSONS LUG_BOOT SAFETY  CLASS\n",
            "0      37       vhigh      4       2      med    low  unacc\n",
            "1      58        high      2    more    small   high  unacc\n",
            "2      30         med  5more    more      med    low  unacc\n",
            "3      89       vhigh      4    more    small   high  unacc\n",
            "4      10         low      3       2      big   high  unacc\n",
            "\n",
            "Samples in the Test Dataset:\n",
            "   BUYING MAINTENANCE  DOORS PERSONS LUG_BOOT SAFETY  CLASS\n",
            "0      17       vhigh      3    more    small    med  unacc\n",
            "1      24        high  5more       4    small   high    acc\n",
            "2      10       vhigh      3    more      big    med    acc\n",
            "3      73         med      2       4    small   high    acc\n",
            "4      27        high      3       4    small    med  unacc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "5.   Access Feature Names using columns Attribute:\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9Xpw1xCD2jM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the feature names (column names) of the training dataset\n",
        "feature_names = train_df.columns\n",
        "print(\"Feature Names:\")\n",
        "print(feature_names)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LT9x4hgR2sRD",
        "outputId": "7b44d72f-042e-45aa-ac68-fa338439a0ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Names:\n",
            "Index(['BUYING', 'MAINTENANCE', 'DOORS', 'PERSONS', 'LUG_BOOT', 'SAFETY',\n",
            "       'CLASS'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Access Each Column as a Series:"
      ],
      "metadata": {
        "id": "MfjUc4iy2wby"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Access the 'BUYING' column as a Series using square bracket notation\n",
        "buying_price = train_df['BUYING']\n",
        "print(buying_price.head())\n",
        "\n",
        "# Access the 'MAINTENANCE' column:\n",
        "maintenance_cost = train_df['MAINTENANCE']\n",
        "print(maintenance_cost.head())\n",
        "\n",
        "# Access the 'CLASS' column in the test dataset as a Series\n",
        "labels = train_df['CLASS']\n",
        "print(labels.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGWxsVWr248i",
        "outputId": "fca500b7-6182-4080-9d3c-c405bd8afdca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    37\n",
            "1    58\n",
            "2    30\n",
            "3    89\n",
            "4    10\n",
            "Name: BUYING, dtype: int64\n",
            "0    vhigh\n",
            "1     high\n",
            "2      med\n",
            "3    vhigh\n",
            "4      low\n",
            "Name: MAINTENANCE, dtype: object\n",
            "0    unacc\n",
            "1    unacc\n",
            "2    unacc\n",
            "3    unacc\n",
            "4    unacc\n",
            "Name: CLASS, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Use value_counts() function to  find the number of samples for each distinct value for a particular column:"
      ],
      "metadata": {
        "id": "-JfDTOKe3sTm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Counts of each distinct value in 'BUYING':\")\n",
        "print (maintenance_cost.value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QKLkuPXm46YG",
        "outputId": "07875413-94ce-4019-aa69-35c581cfb0b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counts of each distinct value in 'BUYING':\n",
            "MAINTENANCE\n",
            "high     313\n",
            "med      299\n",
            "low      298\n",
            "vhigh    290\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Questions\n",
        "---\n",
        "\n",
        "## - Part 1: Check Terminal Node Condition:\n",
        "(Q.1. **5 Marks**): In the first step, we need to check if a node containing a DataFrame is a terminal node or it needs further splitting. Implement a function called `'check_if_terminal'` to do this task.\n",
        "\n",
        "Function Requirements:\n",
        "\n",
        "Input:\n",
        "\n",
        "\n",
        "*   `'parent_data'`: the DataFrame corresponding to a node.\n",
        "\n",
        "*   `'threshold'`: Proportion threshold for the majority class.\n",
        "\n",
        "\n",
        "\n",
        "Calculate the proportion of samples with the majority class label.\n",
        "\n",
        "If the proportion â‰¥ threshold, return \"Leaf\" as flag.\n",
        "\n",
        "If the proportion < threshold, return \"Internal\" as the flag.\n",
        "\n",
        "In addition to the flag, the function must return majority class (\"acc\"/\"unacc\"/\"good\", \"vgood\")"
      ],
      "metadata": {
        "id": "ueIDC_tn_lLu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_if_terminal(dataframe, threshold):\n",
        "  # Get all attribute names from the DataFrame\n",
        "  all_attrs = dataframe.columns\n",
        "\n",
        "  # Select the last attribute as the class attribute\n",
        "  class_attrs = all_attrs[-1]\n",
        "\n",
        "  # Extract the labels (values of the class attribute)\n",
        "  labels = dataframe[class_attrs]\n",
        "  #...............................................\n",
        "  # write the rest here\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # output flag must be a string (whether \"Internal\" or \"Leaf\")\n",
        "  # majority_class must be a string indicating the majority label of the samples in the node\n",
        "  #................................................\n",
        "  return flag, majority_class"
      ],
      "metadata": {
        "id": "dkKqk351BFbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check your implementation on training dataframe:\n",
        "flag, majority_class = check_if_terminal(train_df, 0.9)\n",
        "print(\"the node type is {}\".format(flag))\n",
        "print(\"the majority class of the node is {}\".format(majority_class))"
      ],
      "metadata": {
        "id": "xyHOFah3-1Oh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "## - Part 2: Gini Function:\n",
        "(Q.2. **5 Marks**): In order to split a node in a decision tree based on the Gini index criterion, we need to calculate the Gini impurity of the samples. Gini impurity is a measure of how often a randomly chosen element from the set would be incorrectly classified if it was randomly labeled according to the distribution of labels in the set.\n",
        "\n",
        "Task: Write a Python function called `'gini'` that takes the CLASS column of the dataframe, denoted as \"labels,\" and returns the Gini impurity as the output.\n"
      ],
      "metadata": {
        "id": "vTq3CiKq5SFS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def gini(labels):\n",
        "  # write your code here\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  return gini_impurity\n"
      ],
      "metadata": {
        "id": "MkZoOwiZ7DMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check your implementation on training dataframe:\n",
        "labels = train_df[\"CLASS\"]\n",
        "gini_value = gini(labels)\n",
        "print(\"gini index of the node is {}\".format(gini_value))"
      ],
      "metadata": {
        "id": "YAj5YHcz_bWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "##  - Part 3: Gini Gain Calculation:\n",
        "\n",
        "(Q.3. **15 Marks**): In this step, you are required to implement a function named **`gini_gain`** that computes the **Gini gain** (i.e., the reduction in Gini impurity) obtained by splitting samples in the `'CLASS'` column (referred to as `'labels'`) based on a specific attribute column (denoted as `'x'`). Both `'labels'` and `'x'` are columns in a given DataFrame. You should utilize the **`gini`** function implemented in Part 2 to help compute the Gini gain.\n",
        "\n",
        "#### Key Requirements:\n",
        "- **Categorical Attributes**: If the attribute column `'x'` is categorical, the function should calculate and return the Gini gain achieved by splitting based on the unique values of the attribute.\n",
        "- **Numerical Attributes**: If the attribute column `'x'` is numerical, the function should not only compute the Gini gain but also determine the optimal split point for the attribute. The function should return both the Gini gain and the corresponding optimal split point.\n",
        "\n",
        "#### Notes:\n",
        "- The function should be versatile enough to handle both categorical and numerical attributes.\n",
        "- You may find it helpful to sort the numerical attribute values to determine potential split points.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sIUqZqOl8fie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gini_gain(x, labels):\n",
        "  # Get the number of samples in the dataset\n",
        "  num_samples = len(labels)\n",
        "\n",
        "\n",
        "  # Calculate the gini index of the entire dataset (parent gini)\n",
        "  parent_gini = gini(labels)\n",
        "\n",
        "  # Determine the attribute type based on its dtype and cardinality\n",
        "  if x.dtype == \"object\":\n",
        "    attr_type = \"categorical\"\n",
        "  else:\n",
        "    attr_type = \"numerical\"\n",
        "\n",
        "  # Calculate gini gain based on the attribute type\n",
        "  if attr_type == \"categorical\":\n",
        "    # For categorical attributes, calculate gini gain\n",
        "    # by considering each unique value separately\n",
        "    values = x.unique()\n",
        "    gini_list = []\n",
        "    portion_list = []\n",
        "    for val in values:\n",
        "      pass\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  else:\n",
        "    pass\n",
        "    # For numerical attributes, calculate gini gain\n",
        "    # by considering different split points\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # Calculate gini gain as the difference between parent and child gini index\n",
        "  ginigain = parent_gini - childs_gini\n",
        "\n",
        "  # Return gini gain and the split point (if numerical)\n",
        "  return ginigain, split_point"
      ],
      "metadata": {
        "id": "izpdQGkh-PaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check your implementation for training dataframe on \"PERSONS\" attribute:\n",
        "labels = train_df[\"CLASS\"]\n",
        "x = train_df[\"PERSONS\"]\n",
        "ginigain, split_point = gini_gain(x, labels)\n",
        "print(\"gini gain of the node in splitting over PERSONS attribute is {}\".format(ginigain))\n",
        "#split point is None here\n",
        "##############################################\n",
        "\n",
        "# Check your implementation for training dataframe on \"BUYING\" attribute:\n",
        "labels = train_df[\"CLASS\"]\n",
        "x = train_df[\"BUYING\"]\n",
        "ginigain, split_point = gini_gain(x,labels)\n",
        "print(\"gini gain of the node in splitting over BUYING attribute is {} and the splitting point is {}\".format(ginigain, split_point))"
      ],
      "metadata": {
        "id": "_SbID8sz_uv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "\n",
        "## - Part 4: Selecting the Best Attribute for Splitting\n",
        "(Q.4. **5 Marks**): In this part, you are tasked with implementing a function called  `'select_attribute'`. This function will take a parent DataFrame referenced as `'parent_data'` along with a list of splittable attributes denoted by '`remaining_attrs'` as the input and returns a string representing name of the best attribute which yields to the highest gini gain after splitting. In addition, if the selected attribute is numeric, the function must return the best splitting point too. Otherwise return None. You may use the function written in \"Part 3\".\n",
        "\n"
      ],
      "metadata": {
        "id": "w_C2rfRQPNxE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def select_attribute(parent_data, remaining_attrs):\n",
        "  all_attrs = parent_data.columns\n",
        "  # Extract the class attribute:\n",
        "  class_attr = all_attrs[-1]\n",
        "\n",
        "  # Extract the labels (target values) from the parent data\n",
        "  labels = parent_data[class_attr]\n",
        "\n",
        "\n",
        "\n",
        "  # Loop through independent attributes and calculate their gini gains\n",
        "  # ....................................................\n",
        "  # write the rest here\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # ....................................................\n",
        "  # if the selected attribute is categorical, return None for the sel_split_point\n",
        "  return sel_attr, sel_split_point\n"
      ],
      "metadata": {
        "id": "PrRSD7ezQyyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check your implementation on training dataframe:\n",
        "remaining_attrs = list(train_df.columns[:-1])\n",
        "sel_attr, sel_split_point = select_attribute(train_df, remaining_attrs)\n",
        "print(\"the best attribute for splitting the node is {}\".format(sel_attr))"
      ],
      "metadata": {
        "id": "8SPm6-CnAgfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "---\n",
        "# - Part 5: Splitting the nodes at each tree level\n",
        "\n",
        "(Q.5. **25 Marks**): In this assignment, you will be implementing a crucial part of the decision tree implementation by creating a Python function called `'data_split'`. The purpose of this function is to split a parent node's dataframe into child dataframes based on the best attribute, which yields the highest gini gain. You may use the helper functions that you have already implemented in previous sections.\n",
        "\n",
        "\n",
        "**Instructions:**\n",
        "* Write a function called `'data_split'` to split all the nodes in level \"n\" and to generate all the children nodes in level \"n+1\".\n",
        "\n",
        "* Perform node splitting in a systematic manner, progressing level by level. This entails creating all nodes at level n+1 by dividing all nodes eligible for splitting at level n. Refer to the example below for clarification:\n",
        "\n",
        "![Image](https://drive.google.com/uc?export=download&id=1kIOCkYaxUJMEKumBP6RxOLriQQY2Wlqx)\n",
        " As depicted in the illustration, at level 1, there is a solitary node designated as \"Node_1_1,\" symbolizing the first node of the first level. Level 1 has been subdivided into three nodes, identified as \"Node_2_1,\" \"Node_2_2,\" and \"Node_2_3,\" signifying the first, second, and third nodes of the second level of splitting. Please adhere to this notation for naming each node.\n",
        "\n",
        "* Imagine a dictionary named `'dataframe_dict'`, where the \"keys\" correspond to the node names at a specific splitting level, and the \"values\" represent the associated dataframes. To illustrate, for level 1, the `'dataframe_dict'` would consist of a single key, \"Node_1_1,\" with the corresponding value being the primary dataframe:\n",
        "                dataframe_dict = {\"Node_1_1\": the main dataframe}\n",
        "In this example, following the execution of the `'data_split'` function, the `'dataframe_dict'` dictionary should be replaced with a dictionary containing three entries, as demonstrated below:\n",
        "      dataframe_dict = {\n",
        "                          \"Node_2_1\": dataframe_2_1,\n",
        "                          \"Node_2_2\": dataframe_2_2,\n",
        "                          \"Node_2_3\": dataframe_2_3\n",
        "                        }\n",
        "\n",
        "\n",
        "* Similarly, consider another dictionary called `'remaining_attrs'` with \"keys\" representing the nodes' names, and \"values\" representing the splittable attributes for each node. In this example, the dictionary at first level would be:\n",
        "\n",
        "      remaining_attrs = {\"Node_1_1\": ['BUYING', 'MAINTENANCE', 'DOORS', 'PERSONS', 'LUG_BOOT', 'SAFETY']}\n",
        "but after running the function `'data_split'`, it would be updated to a dictionary with three keys-values as:\n",
        "\n",
        "     \n",
        "      remaining_attrs = {\n",
        "                         \"Node_2_1\": ['BUYING', 'MAINTENANCE', 'DOORS', 'LUG_BOOT', 'SAFETY'],\n",
        "                         \"Node_2_2\": ['BUYING', 'MAINTENANCE', 'DOORS', 'LUG_BOOT', 'SAFETY'] ,\n",
        "                         \"Node_2_3\": ['BUYING', 'MAINTENANCE', 'DOORS', 'LUG_BOOT', 'SAFETY']\n",
        "                         }\n",
        "\n",
        "Please note that once we've performed a split on a categorical attribute such as \"PERSONS\" and generated the children nodes in the subsequent level, we are no longer permitted to split on the same categorical attribute within that branch of the tree. It's important to emphasize that this restriction doesn't apply to numerical attributes.\n",
        "\n",
        "In the context of this example, this means that the `'remaining_attrs'` dictionary is updated to a three-element dictionary, where none of the nodes in this specific branch have the \"PERSONS\" attribute as a splittable option anymore.\n",
        "\n",
        "* Consider the `'tree_model'` as a list containing three additional dictionaries: `'tree_connectivity'`, `'node_labels'`, `'and node_types'`:\n",
        "\n",
        "            tree_model = [tree_connectivity , node_types, node_labels]\n",
        "\n",
        "where `'tree_connectivity'` is a dictionary representing the node connection to the parents. The `'node_types'` and `'node_labels'` are also dictionaries containing the (\"Leaf\" or \"Internal\") and the majority class for each node, respectively. Your `'data_split'` function must take the `'tree_model'` generated up to  level \"n\" and must update it to the model up top level \"n+1\" after splitting. See the below image for this example:\n",
        "The `'tree_model'` at level 1 is:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=download&id=1GSJzh4CNE298LFXQR86LYpEfj4Q883-S\"  width=400>\n",
        "\n",
        "\n",
        "After running the `'data_split'` function, the tree_model will be updated up to level 2 as follows:\n",
        "\n",
        "![Image](https://drive.google.com/uc?export=download&id=1Y3sGXHBpQtPVpMh8UnVlO0AYXvHNTGfo)\n",
        "\n",
        "\n",
        "**Therefore**: You must write the function `'data_split'` which takes `'dataframe_dict'`, `'remaining_attrs'`, `'tree_model'`, `'level'`, and `'threshold'` as the input and update `'dataframe_dict'`, `'remaining_attrs'`, and `'tree_model'` upto level \"level+1\". The function must also retun a boolean flag `'stop_train'` which must be True if any child node is generated. Otherwise, it must return False. Here, input `'threshold'` is the majority class threshold for checking wether a node is a \"Leaf\" node or an \"Internal\" node.\n",
        "\n",
        "**To complete the function**:\n",
        "* Loop through the nodes in \"dataframe_dict\" in the current \"level\". For each node, check if it's an \"Internal\" node and if so, find the best attribute for splitting. Create child nodes and finally update all the variables.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1UH1AkKUrCI8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_split(dataframe_dict, remaining_attrs, tree_model, level, threshold):\n",
        "    \"\"\"\n",
        "    Splits data at each node and updates the decision tree model.\n",
        "\n",
        "    Args:\n",
        "    dataframe_dict (dict): Current node's data.\n",
        "    remaining_attrs (dict): Remaining attributes for each node.\n",
        "    tree_model (list): Tree structure (connectivity, labels, types).\n",
        "    level (int): Current depth level.\n",
        "    threshold (float): Class purity threshold for pre-pruning.\n",
        "\n",
        "    Returns:\n",
        "    tree_model (list): Updated tree structure.\n",
        "    stop_train (bool): Indicates whether to stop the training.\n",
        "    dataframe_dict (dict): Updated data for child nodes.\n",
        "    remaining_attrs (dict): Updated attributes for child nodes.\n",
        "    \"\"\"\n",
        "\n",
        "    # Unpack tree model\n",
        "    [tree_connectivity, node_labels, node_types] = tree_model\n",
        "\n",
        "    # Initialize child node counter\n",
        "    child_ind = 1\n",
        "\n",
        "    dataframe_dict_new = {}\n",
        "    remaining_attrs_new = {}\n",
        "\n",
        "    # Iterate over parent nodes\n",
        "    for key in dataframe_dict.keys():\n",
        "        parent_data = dataframe_dict[key]\n",
        "        candidate_attrs = remaining_attrs[key].copy()\n",
        "\n",
        "        # Check if the node is \"Internal\"\n",
        "        if node_types[key] == \"Internal\":\n",
        "            # TODO: Select best attribute (use select_attribute function)\n",
        "\n",
        "\n",
        "            # TODO: Handle distinct values or split points\n",
        "            # (Hint: use .unique() for categorical, split into <= and > for numerical)\n",
        "\n",
        "            child_dict = {}\n",
        "\n",
        "            # TODO: Loop through distinct values or split points\n",
        "            for value in []:  # Replace with appropriate logic\n",
        "                child_name = f\"node_{level + 1}_{child_ind}\"\n",
        "\n",
        "                # TODO: Create mask for filtering data\n",
        "\n",
        "\n",
        "                # TODO: Store filtered data\n",
        "                # child_data = parent_data[mask]\n",
        "                # dataframe_dict_new[child_name] = child_data\n",
        "\n",
        "                # TODO: Check if child node is terminal (use check_if_terminal)\n",
        "\n",
        "\n",
        "                # TODO: Update node labels, types, and tree connectivity\n",
        "\n",
        "\n",
        "                child_ind += 1\n",
        "\n",
        "            # tree_connectivity[key] = child_dict\n",
        "\n",
        "    # Update dataframe_dict and remaining_attrs\n",
        "    # Return updated tree model and stop flag\n",
        "    stop_train = (child_ind == 1)\n",
        "\n",
        "    return tree_model, stop_train, dataframe_dict, remaining_attrs\n"
      ],
      "metadata": {
        "id": "bAsxLUcTrFtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now Check your implementation on training dataframe:\n",
        "# Initializing\n",
        "threshold = 0.9\n",
        "\n",
        "tree_connectivity = {}\n",
        "\n",
        "flag, majority_class = check_if_terminal(train_df, 0.9)\n",
        "\n",
        "node_types = {\"node_1_1\": flag}\n",
        "node_labels = {\"node_1_1\": majority_class}\n",
        "\n",
        "# Create an initial tree_model\n",
        "tree_model = [tree_connectivity, node_labels, node_types]\n",
        "\n",
        "# Create an initial dataframe_dict\n",
        "dataframe_dict = {\"node_1_1\": train_df}\n",
        "\n",
        "# Create an initial remaining_attrs\n",
        "independent_attrs = list(train_df.columns[:-1])\n",
        "remaining_attrs = {\"node_1_1\": independent_attrs}\n",
        "\n",
        "# Set level to 1\n",
        "level = 1\n",
        "\n",
        "\n",
        "# Update tree model\n",
        "tree_model, stop_train, dataframe_dict, remaining_attrs = data_split(dataframe_dict, remaining_attrs, tree_model, 1, threshold)\n",
        "tree_model, stop_train, dataframe_dict, remaining_attrs = data_split(dataframe_dict, remaining_attrs, tree_model, 2, threshold)\n",
        "tree_model, stop_train, dataframe_dict, remaining_attrs = data_split(dataframe_dict, remaining_attrs, tree_model, 3, threshold)\n",
        "\n",
        "\n",
        "[tree_connectivity, node_labels, node_types] = tree_model\n",
        "\n",
        "print(\"\\n tree connectivity:\")\n",
        "print(tree_connectivity)\n",
        "\n",
        "print(\"\\n node labels:\")\n",
        "print(node_labels)\n",
        "\n",
        "print(\"\\n node types:\")\n",
        "print(node_types)\n",
        "\n",
        "print(\"\\n remaining attributes are:\")\n",
        "print(remaining_attrs)\n",
        "\n"
      ],
      "metadata": {
        "id": "TbJDC_sFA5jm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## -Part 6: Training the Decision Tree\n",
        "\n",
        "(Q.6. **10 Marks**): Now, let's create a function called `'tree_train'` to train the decision tree. This function begins by initializing the tree model and dataframe dictionary using the root node named \"node_1_1.\" It then iteratively updates these structures as it progresses through the tree, continuing until no further child nodes are generated. The process starts at level 1, and with each iteration, the level is incremented. Importantly, make sure to utilize the `'data_split'` function, which you have previously implemented, to assist in the tree construction. Ultimately, the function must return the fully trained tree model.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-K2ROrMYyUfR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tree_train(training_data, threshold):\n",
        "  # Initializing\n",
        "  tree_connectivity = {}\n",
        "\n",
        "  flag, majority_class = check_if_terminal(training_data, threshold)\n",
        "\n",
        "  node_types = {\"node_1_1\": flag}\n",
        "  node_labels = {\"node_1_1\": majority_class}\n",
        "\n",
        "  # Create a tree_model list to store connectivity, node labels, and node types\n",
        "  tree_model = [tree_connectivity, node_labels, node_types]\n",
        "\n",
        "  # Create a dataframe_dict with the initial training data and associate it with the root node\n",
        "  dataframe_dict = {\"node_1_1\": training_data}\n",
        "\n",
        "   # Create a remaining_attrs dictionary with all the independent attributes and associate it with the root node\n",
        "  indp_attrs = list(training_data.columns[:-1])\n",
        "  remaining_attrs = {\"node_1_1\": indp_attrs}\n",
        "\n",
        "  # Initialize the level of the tree to 1\n",
        "  level = 1\n",
        "\n",
        "  # Continue tree construction until a stopping condition is met\n",
        "  # ...............................................................\n",
        "  # write the rest here\n",
        "  # write a loop function and exit the loop if terminating criterion is met\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #.....................................................................\n",
        "  return tree_model"
      ],
      "metadata": {
        "id": "1EVuxiv2zD8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check your implementation on training dataframe:\n",
        "tree_model = tree_train(train_df, 0.9)\n",
        "[tree_connectivity, node_labels, node_types] = tree_model\n",
        "\n",
        "print(\"\\n tree connectivity:\")\n",
        "print(tree_connectivity)\n",
        "\n",
        "print(\"\\n node labels:\")\n",
        "print(node_labels)\n",
        "\n",
        "print(\"\\n node types:\")\n",
        "print(node_types)\n"
      ],
      "metadata": {
        "id": "cHS5ytxLDEdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# Part 7: Prediction by the Desicion Tree\n",
        "(Q.7. **15 Marks**): Following the completion of decision tree training, the next step is to implement the prediction process through the trained tree structure. To achieve this, we need to create a function named `'tree_prediction'`. This function takes two inputs: a test dataframe containing the samples to be predicted and the trained decision tree. It returns the predicted labels generated by the decision tree as a single DataFrame column."
      ],
      "metadata": {
        "id": "ty1X5YsJ25UT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tree_prediction(testing_data, tree_model):\n",
        "    # Initialize a list to store the predicted labels\n",
        "    pred_labels = []\n",
        "\n",
        "    # Unpack the tree_model list into three separate variables: tree_connectivity, node_labels, and node_types\n",
        "    [tree_connectivity, node_labels, node_types] = tree_model\n",
        "\n",
        "    # Iterate through each sample in the testing_data\n",
        "    for i in range(len(testing_data)):\n",
        "        # Get a sample from the testing dataset\n",
        "        sample = testing_data.loc[i]\n",
        "\n",
        "        # Start at the root node, which is always named \"node_1_1\"\n",
        "        current_node = \"node_1_1\"\n",
        "\n",
        "        # Begin a loop to traverse the decision tree until a leaf node is reached\n",
        "        #........................................................................\n",
        "        # write the rest here:\n",
        "        # Begin a loop to traverse the decision tree until a leaf node is reached\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # find the node label and put it in the pred_labels Pandas Series\n",
        "    #.............................................................................\n",
        "    # Return the Pandas Series containing the predicted labels\n",
        "    return pred_labels\n"
      ],
      "metadata": {
        "id": "0Yi1bTnb3rEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check your implementation on training dataframe:\n",
        "tree_model = tree_train(train_df, 0.9)\n",
        "pred_labels = tree_prediction(train_df, tree_model)\n",
        "print(pred_labels)"
      ],
      "metadata": {
        "id": "gtIABZziDgSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Part 8: Evaluating the Model\n",
        "(**10 Marks**)\n",
        "* (Q.8-a.)In the final step of this assignment, you'll apply the decision tree learning process. Start by training the decision tree on the training dataset using the `'tree_train'` function, setting the terminating threshold to 0.9. Next, employ the `'tree_prediction'` function, as previously implemented, to generate predictions for both the training and testing datasets. Following this, your task is to compare these predicted labels with the actual ground-truth labels to compute and report the accuracy rates for both the training and testing datasets.\n",
        "\n",
        "* (Q.8-b) Now, repeat the process with a different terminating threshold, specifically 0.7, and once again calculate and report the accuracy rates for the training and testing datasets. Finally, compare and contrast the results obtained with the two different threshold values (0.9 and 0.7)."
      ],
      "metadata": {
        "id": "uRVHjnEi5lvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#.................................\n",
        "# write the rest here:\n",
        "def accuracy_cal(true_labels, pred_labels):\n",
        "  #write you code here\n",
        "\n",
        "\n",
        "\n",
        "  return acc\n"
      ],
      "metadata": {
        "id": "D9Jt2EBp6uuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"train accuracy with threshold= 0.9 is: {}\".format(train_acc))\n",
        "print(\"test accuracy with threshold= 0.9 is: {}\".format(test_acc))\n",
        "\n",
        "print(\"\\n train accuracy with threshold= 0.7 is: {}\".format(train_acc))\n",
        "print(\"test accuracy with threshold= 0.7 is: {}\".format(test_acc))"
      ],
      "metadata": {
        "id": "V5ovT7R67VX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Part 9: Post-Pruning (Reduced Error Pruning):\n",
        "\n",
        "(Q.9. **10 Marks**): Post-pruning involves simplifying a fully grown decision tree by removing unnecessary internal nodes that do not contribute to better performance. The goal is to reduce overfitting while maintaining or improving accuracy on unseen data.\n",
        "\n",
        "- Implement the reduced error post-pruning algorithm.\n",
        "  - The algorithm should take a fully grown tree (represented by `tree_model = [tree_connectivity, node_types, node_labels]`) and a validation dataset.\n",
        "  \n",
        "- Follow this process:\n",
        "  - Start from the bottom of the tree.\n",
        "  - For each internal node, check whether converting that node to a leaf improves validation accuracy.\n",
        "  - If converting the node to a leaf increases validation accuracy, keep it as a leaf. Otherwise, retain it as an internal node.\n",
        "\n",
        "- Progressively scan all nodes from the bottom to the top of the tree.\n",
        "  - You do not need to remove the connections in `tree_connectivity`; simply change the node type to \"leaf.\"\n",
        "  - Scanning will stop once a terminal (leaf) node is reached.\n",
        "\n",
        "- After post-pruning the tree, evaluate the pruned tree on the test dataset and report the test accuracy.\n",
        "\n"
      ],
      "metadata": {
        "id": "XOeMmX6m7oYd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define the function here\n",
        "def post_pruning(tree_model, validation_df):\n",
        "    [tree_connectivity, node_labels, node_types] = tree_model\n",
        "    #write your code here\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return tree_model"
      ],
      "metadata": {
        "id": "hHDxNgf37xeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# call the function here\n",
        "all_attrs = train_df.columns\n",
        "class_attr = all_attrs[-1]\n",
        "tree_model = tree_train(train_df, 1)\n",
        "tree_model_new = post_pruning(tree_model, validation_df)\n",
        "test_pred_label = tree_prediction(test_df, tree_model_new)\n",
        "test_acc = accuracy_cal(test_df[class_attr], test_pred_label)\n",
        "print(\"test accuracy with threshold= 1 is: {}\".format(test_acc))"
      ],
      "metadata": {
        "id": "U6n_Rb_H7z12"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}